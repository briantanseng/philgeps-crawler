# PhilGEPS Crawler Configuration
# Copy this file to .env and update with your settings

# Environment
NODE_ENV=development
# Options: development, production

# Base URL for PhilGEPS
# Default search page with Result=3 filter for active opportunities
PHILGEPS_BASE_URL=https://notices.philgeps.gov.ph/GEPSNONPILOT/Tender/SplashOpportunitiesSearchUI.aspx?menuIndex=3&ClickFrom=OpenOpp&Result=3

# Database configuration
DATABASE_TYPE=sqlite3
# Options: sqlite3, postgres

# SQLite configuration (when DATABASE_TYPE=sqlite3)
DATABASE_PATH=./data/philgeps.db
# For production, consider using absolute path or cloud storage

# PostgreSQL configuration (when DATABASE_TYPE=postgres)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DATABASE=philgeps
POSTGRES_SSL=false
# For production, use DATABASE_URL instead:
# DATABASE_URL=postgresql://user:password@host:port/database?sslmode=require

# Crawler settings
CRAWL_INTERVAL_MINUTES=60
# How often to run automatic crawls (in minutes)

MAX_CONCURRENT_REQUESTS=5
# Maximum number of concurrent HTTP requests

REQUEST_DELAY_MS=3000
# Delay between requests in milliseconds (to respect rate limits)
# Increase this value if you get connection errors (try 5000 or 10000)

MAX_PAGES_TO_CRAWL=20
# Maximum pages to crawl in one session (for crawl:all command)

CRAWL_START_PAGE=1
# Starting page number for crawling (default: 1)

CRAWL_END_PAGE=
# Ending page number for crawling (default: MAX_PAGES_TO_CRAWL)

RUN_INITIAL_CRAWL=false
# Whether to run a crawl when the application starts
# Set to true for production if you want immediate data

# Full download settings (for download:all script)
DOWNLOAD_BATCH_SIZE=10
# Number of pages to download in each batch

MAX_JITTER_MS=3000
# Maximum random delay to add (milliseconds)

BATCH_PAUSE_MS=30000
# Pause between batches (milliseconds)

TOTAL_PAGES=1200
# Estimated total pages (will be auto-detected if possible)

AUTO_CONFIRM=false
# Skip confirmation prompt for automated downloads

PAGE_RETRY_ATTEMPTS=3
# Number of retry attempts for failed pages

PAGE_RETRY_MULTIPLIER=2
# Multiplier for retry delay (exponential backoff)

# Server configuration
PORT=3000
# Port for the web server (Cloud Run uses PORT env var)

# Puppeteer configuration (for Docker/Cloud Run)
PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
# Set to true when using system-installed Chromium

PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
# Path to Chromium executable (for Docker environments)

# API configuration
API_RATE_LIMIT_WINDOW=15
# Rate limit window in minutes

API_RATE_LIMIT_MAX_REQUESTS=100
# Maximum requests per window

# CORS configuration
CORS_ORIGINS=*
# Comma-separated list of allowed origins, or * for all
# Example: https://example.com,https://app.example.com

# Logging configuration
LOG_LEVEL=info
# Options: error, warn, info, debug

LOG_FORMAT=json
# Options: json, simple

# Feature flags
ENABLE_FULL_CRAWL=true
# Enable/disable full crawl functionality (crawl:all command)

ENABLE_SCHEDULED_CRAWL=true
# Enable/disable automatic scheduled crawling

ENABLE_API=true
# Enable/disable REST API endpoints

ENABLE_WEB_UI=true
# Enable/disable web UI

# Performance tuning
CRAWLER_TIMEOUT_SECONDS=3600
# Maximum time for a crawl operation (1 hour default)

SEARCH_RESULTS_LIMIT=1000
# Maximum search results to return

EXPORT_CSV_LIMIT=5000
# Maximum records to export in CSV

# Cloud-specific settings (for Google Cloud Run)
GOOGLE_CLOUD_PROJECT=
# Your Google Cloud project ID

GOOGLE_CLOUD_REGION=asia-southeast1
# Deployment region

CLOUD_STORAGE_BUCKET=
# GCS bucket for database backups (optional)

# Monitoring and alerts
SENTRY_DSN=
# Sentry error tracking DSN (optional)

SLACK_WEBHOOK_URL=
# Slack webhook for notifications (optional)

# Security
API_KEY=
# Optional API key for protected endpoints

ADMIN_PASSWORD=
# Password for admin endpoints (optional)

# Development settings
DEBUG=false
# Enable debug mode (verbose logging)

FORCE_COLORS=true
# Force colored output in logs