# PhilGEPS Crawler

A modern Next.js application for searching and crawling Philippine Government Electronic Procurement System (PhilGEPS) opportunities with advanced features and real-time data management.

## ğŸš€ Key Features

### Search & Discovery
- ğŸ” **Advanced Search** - Full-text search with keyword matching across titles and entities
- ğŸ·ï¸ **Smart Filtering** - Filter by category, area of delivery, budget range, and status
- ğŸ“Š **Real-time Results** - Instant search results with pagination and sorting
- ğŸ“¥ **CSV Export** - Export search results with all filters applied

### Data Visualization
- ğŸ“ˆ **Live Statistics Dashboard** - Real-time metrics showing:
  - Total opportunities in database
  - Active opportunities count
  - Number of categories
  - Procuring entities count
- ğŸ¯ **Status Indicators** - Visual badges for opportunity status (Active, Closing Soon, Expired)
- â±ï¸ **Days Until Closing** - Countdown display for each opportunity

### ITB Details & Expansion
- ğŸ“„ **Expandable Rows** - Interactive rows with smooth animations
  - Visual arrow indicator (â–¶/â–¼) shows expandable rows
  - "ITB" badge indicates rows with available details
  - Smooth slide-in animation when expanding/collapsing
  - State persists during pagination
- ğŸ“‹ **Comprehensive ITB Information** including:
  - Solicitation details and procurement mode
  - Funding source and delivery period
  - Contact information (person, email, phone, address)
  - Pre-bid conference details
  - Bid submission and opening dates
  - Trade agreements and classifications
  - BAC (Bids and Awards Committee) information
  - Eligibility requirements and descriptions
  - Direct links to PhilGEPS pages
- ğŸ”„ **Integrated ITB Crawling** - Automatic ITB details extraction during main crawl

### Crawler Management
- ğŸ¤– **Enhanced Crawler v2.0** - Intelligent crawling with ITB integration
  - Automatic ITB details extraction for each opportunity
  - Batch processing with configurable batch size
  - Retry logic with exponential backoff
  - State management for resume capability
  - Comprehensive error logging and recovery
- ğŸ›ï¸ **Web-based Crawler Control Panel**:
  - Enable/disable crawler with toggle switch
  - Manual crawl triggering with "Run Now" button
  - Real-time crawler status (Active/Inactive/Running)
  - Auto-refreshing status every 10 seconds
- ğŸ“Š **Crawl History & Metrics**:
  - Last crawl timestamp and duration
  - Opportunities found, new, and updated counts
  - ITB details fetched count
  - Error tracking and status reporting
  - Success rate and performance metrics

### Technical Features
- âš¡ **Next.js 14 App Router** - Modern React framework with server components
- ğŸ—„ï¸ **SQLite Database** - Efficient local storage with automatic schema migration
- ğŸ¨ **Tailwind CSS** - Responsive, mobile-first design
- ğŸ“± **Fully Responsive** - Optimized for desktop, tablet, and mobile devices
- ğŸ§ª **Comprehensive Testing** - E2E tests with Playwright across multiple browsers
- ğŸ”„ **Real-time Updates** - Automatic UI updates without page refresh

## ğŸ“‹ Prerequisites

- Node.js 18+ 
- npm or yarn
- Chrome/Chromium (for Puppeteer-based crawling)

## ğŸ› ï¸ Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd philgeps-crawler
```

2. **Install dependencies**
```bash
npm install
```

3. **Set up environment variables**
```bash
cp .env.example .env
```

4. **Initialize the database**
```bash
npm run migrate
```

5. **Start the development server**
```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to view the application.

## ğŸ“ Project Structure

```
philgeps-crawler/
â”œâ”€â”€ app/                        # Next.js 14 App Directory
â”‚   â”œâ”€â”€ api/                   # API Routes
â”‚   â”‚   â”œâ”€â”€ areas/            # Area reference data
â”‚   â”‚   â”œâ”€â”€ categories/       # Category reference data
â”‚   â”‚   â”œâ”€â”€ crawler/          # Crawler control endpoints
â”‚   â”‚   â”œâ”€â”€ opportunities/    # Search and export
â”‚   â”‚   â””â”€â”€ statistics/       # Dashboard metrics
â”‚   â”œâ”€â”€ layout.tsx            # Root layout with metadata
â”‚   â””â”€â”€ page.tsx              # Home page with search UI
â”œâ”€â”€ components/                # React Components
â”‚   â”œâ”€â”€ CrawlerControl.tsx    # Crawler management panel
â”‚   â”œâ”€â”€ OpportunitiesTable.tsx # TanStack Table with ITB expansion
â”‚   â”œâ”€â”€ SearchForm.tsx        # Advanced search form
â”‚   â””â”€â”€ Statistics.tsx        # Metrics dashboard
â”œâ”€â”€ lib/                      # Core Libraries
â”‚   â”œâ”€â”€ db.ts                # Database connection
â”‚   â”œâ”€â”€ crawler/             # Crawler service
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â””â”€â”€ searchService.ts # Search and formatting
â”‚   â””â”€â”€ utils/               # Utilities
â”‚       â””â”€â”€ dateFormat.ts    # Date/number formatting
â”œâ”€â”€ src/                     # Legacy Crawler Code
â”‚   â”œâ”€â”€ scrapers/           # Web scraping logic
â”‚   â”‚   â”œâ”€â”€ PhilGEPSScraper.js    # Main scraper
â”‚   â”‚   â”œâ”€â”€ ITBDetailScraper.js   # ITB details
â”‚   â”‚   â””â”€â”€ PuppeteerScraper.js   # Browser automation
â”‚   â”œâ”€â”€ services/           # Crawler services
â”‚   â””â”€â”€ migrations/         # Database schema
â”œâ”€â”€ tests/                  # Test Suites
â”‚   â””â”€â”€ e2e/               # Playwright E2E tests
â”œâ”€â”€ public/                # Static assets
â””â”€â”€ data/                  # SQLite database (auto-created)
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file with the following:

```env
# Database
DATABASE_PATH=./data/philgeps.db

# Crawler Configuration
CRAWL_INTERVAL_MINUTES=60         # How often to run crawler
MAX_PAGES_TO_CRAWL=10            # Pages per crawl run
REQUEST_DELAY_MS=2000            # Delay between requests
CRAWLER_TIMEOUT_SECONDS=300      # Overall timeout

# Enhanced Crawler Settings
BATCH_SIZE=10                    # Process opportunities in batches
MAX_RETRIES=3                    # Retry failed requests
BASE_DELAY=2000                  # Base delay between requests (ms)
MAX_JITTER=1000                  # Random jitter added to delays
ITB_FETCH_DELAY=1500             # Delay between ITB detail fetches
ITB_MAX_JITTER=500               # Jitter for ITB fetches
PAGE_CRAWL_TIMEOUT=60000         # Timeout for page crawl (ms)
ITB_CRAWL_TIMEOUT=30000          # Timeout for ITB details (ms)

# Development
NODE_ENV=development
```

### Available Scripts

```bash
# Development
npm run dev                 # Start Next.js dev server
npm run build              # Build for production
npm start                  # Start production server

# Testing
npm run test               # Run all tests
npm run test:e2e           # Run E2E tests only
npm run test:unit          # Run unit tests only
npm run test:coverage      # Generate coverage report

# Crawler Operations
npm run background:crawler  # Start background crawler service
npm run crawl              # Run one-time crawl (first page only)
npm run crawl:all          # Enhanced crawler with ITB details extraction
npm run crawl:all 50       # Crawl first 50 pages
npm run crawl:all 10 20    # Crawl pages 10-20
npm run crawl:resume       # Resume interrupted crawl from saved state
npm run crawl:clean        # Clear crawler state and start fresh
npm run migrate            # Run database migrations

# Code Quality
npm run lint               # Run ESLint
npm run type-check         # Run TypeScript checks
```

## ğŸŒ API Reference

### Opportunities Endpoints

#### Search Opportunities
```http
GET /api/opportunities/search
```
Query parameters:
- `q` - Search keywords
- `category` - Filter by category
- `areaOfDelivery` - Filter by area
- `minBudget` - Minimum budget
- `maxBudget` - Maximum budget
- `activeOnly` - Show only active opportunities
- `limit` - Results per page (default: 50)
- `offset` - Pagination offset

#### Export to CSV
```http
GET /api/opportunities/export
```
Same query parameters as search. Returns CSV file download.

### Reference Data Endpoints

#### Get Statistics
```http
GET /api/statistics
```
Returns:
```json
{
  "success": true,
  "data": {
    "total": 1000,
    "active": 250,
    "categories": 15,
    "entities": 200
  }
}
```

#### Get Categories
```http
GET /api/categories
```

#### Get Areas
```http
GET /api/areas
```

### Crawler Control Endpoints

#### Get Crawler Status
```http
GET /api/crawler/status
```
Returns current status, last crawl info, and configuration.

#### Toggle Crawler
```http
POST /api/crawler/toggle
Body: { "enabled": true/false }
```

#### Run Manual Crawl
```http
POST /api/crawler/run
```

## ğŸ§ª Testing

The project includes comprehensive E2E tests using Playwright:

```bash
# Run all E2E tests
npm run test:e2e

# Run with UI mode
npm run test:e2e -- --ui

# Run specific test file
npm run test:e2e search.spec.ts

# Generate HTML report
npm run test:e2e -- --reporter=html
```

Test coverage includes:
- Homepage functionality
- Search and filtering
- Crawler controls
- ITB detail expansion
- Export functionality
- Cross-browser compatibility (Chrome, Firefox, Safari, Mobile)

## ğŸš€ Deployment

### Docker Deployment

```bash
# Build image
docker build -t philgeps-crawler .

# Run container
docker run -p 3000:3000 -v $(pwd)/data:/app/data philgeps-crawler
```

### Google Cloud Run

```bash
# Build and deploy
gcloud builds submit --config cloudbuild.yaml
```

### Traditional Server

1. Build the application:
```bash
npm run build
```

2. Start the production server:
```bash
npm start
```

3. Run crawler as a separate process:
```bash
# Using PM2
pm2 start npm --name "philgeps-crawler" -- run background:crawler

# Using systemd (create service file)
# See docs/DEPLOYMENT.md for details
```

### Vercel Deployment

1. Push to GitHub
2. Import project in Vercel
3. Configure environment variables
4. Deploy (Note: crawler needs separate hosting)

## ğŸ“Š Database Schema

The application uses SQLite with the following main tables:

### opportunities
- Core procurement data with 40+ fields
- ITB details integrated into main table
- Comprehensive indexing for performance

### crawl_history
- Tracks all crawler runs
- Stores metrics and error information

See `DATABASE_SCHEMA.md` for complete schema documentation.

## ğŸ” Performance Features

- **Efficient Pagination** - Server-side pagination with TanStack Table
- **Smart Caching** - Browser caching for static assets
- **Optimized Queries** - Database indexes on all searchable fields
- **Lazy Loading** - ITB details loaded only when expanded
- **Debounced Search** - Prevents excessive API calls

## ğŸ›¡ï¸ Security Features

- **Input Sanitization** - All user inputs are sanitized
- **SQL Injection Prevention** - Parameterized queries
- **XSS Protection** - React's built-in XSS prevention
- **CORS Configuration** - Proper CORS headers
- **Rate Limiting** - API rate limiting (configurable)

## ğŸ“± Browser Support

- Chrome/Edge (latest)
- Firefox (latest)
- Safari (latest)
- Mobile browsers (iOS Safari, Chrome Android)

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- PhilGEPS for providing public procurement data
- Next.js team for the amazing framework
- TanStack for the powerful table library
- All contributors and testers